{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a504dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Boosting\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost.callback as xgb_callback\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, regularizers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23c957f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows in train: 13\n",
      "Duplicates removed. New shape: (59598, 24)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../train.csv\")\n",
    "test = pd.read_csv(\"../test.csv\")\n",
    "\n",
    "# Constants\n",
    "TARGET = \"retention_status\"\n",
    "IDCOL = \"founder_id\"\n",
    "\n",
    "# Remove duplicates (on entire train DF)\n",
    "dup_count = train.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows in train: {dup_count}\")\n",
    "if dup_count > 0:\n",
    "    train = train.drop_duplicates().reset_index(drop=True)\n",
    "    print(\"Duplicates removed. New shape:\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95a7ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cols = [\n",
    "    'monthly_revenue_generated', 'funding_rounds_led',\n",
    "    'num_dependents', 'years_with_startup'\n",
    "]\n",
    "\n",
    "def feature_engineer(df):\n",
    "    df = df.copy()\n",
    "    if 'years_with_startup' in df and 'years_since_founding' in df:\n",
    "        df['experience_ratio'] = df['years_with_startup'] / (df['years_since_founding'] + 1e-9)\n",
    "    if 'founder_age' in df and 'years_with_startup' in df:\n",
    "        df['founder_join_age'] = df['founder_age'] - df['years_with_startup']\n",
    "    if 'monthly_revenue_generated' in df and 'funding_rounds_led' in df:\n",
    "        df['revenue_per_round'] = df['monthly_revenue_generated'] / (df['funding_rounds_led'] + 1)\n",
    "\n",
    "    for c in log_cols:\n",
    "        if c in df:\n",
    "            df[f\"log_{c}\"] = np.log1p(df[c])\n",
    "            df.drop(c, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering but keep founder_id and target in train_fe for alignment\n",
    "train_fe = feature_engineer(train.copy())\n",
    "test_fe = feature_engineer(test.copy())\n",
    "\n",
    "# Ensure IDCOL exists in test_fe (for submission). If not, try to copy from test.\n",
    "if IDCOL not in test_fe.columns and IDCOL in test.columns:\n",
    "    test_fe[IDCOL] = test[IDCOL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b892b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw feature shapes: (59598, 25) (14900, 25) (59598,)\n"
     ]
    }
   ],
   "source": [
    "X = train_fe.drop(columns=[TARGET, IDCOL])\n",
    "y = train_fe[TARGET].map({\"Stayed\": 1, \"Left\": 0}).astype(int)\n",
    "X_test_final = test_fe.drop(columns=[IDCOL], errors=\"ignore\")\n",
    "\n",
    "print(\"Raw feature shapes:\", X.shape, X_test_final.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d72687c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [\n",
    "    'years_since_founding', 'founder_age', 'distance_from_investor_hub',\n",
    "    'experience_ratio', 'founder_join_age', 'revenue_per_round',\n",
    "    'log_monthly_revenue_generated', 'log_funding_rounds_led',\n",
    "    'log_num_dependents', 'log_years_with_startup'\n",
    "]\n",
    "\n",
    "binary_cols = [\n",
    "    'working_overtime', 'remote_operations',\n",
    "    'leadership_scope', 'innovation_support'\n",
    "]\n",
    "\n",
    "ordinal_cols = {\n",
    "    'work_life_balance_rating': ['Poor', 'Fair', 'Good', 'Excellent'],\n",
    "    'venture_satisfaction': ['Low', 'Medium', 'High', 'Very High'],\n",
    "    'startup_performance_rating': ['Low', 'Below Average', 'Average', 'High'],\n",
    "    'startup_reputation': ['Poor', 'Fair', 'Good', 'Excellent'],\n",
    "    'founder_visibility': ['Low', 'Medium', 'High', 'Very High'],\n",
    "    'startup_stage': ['Entry', 'Mid', 'Senior'],\n",
    "    'team_size_category': ['Small', 'Medium', 'Large']\n",
    "}\n",
    "\n",
    "ordinal_feature_names = list(ordinal_cols.keys())\n",
    "ordinal_categories = list(ordinal_cols.values())\n",
    "nominal_cols = ['founder_gender', 'founder_role', 'education_background', 'personal_status']\n",
    "\n",
    "# Safety: keep only columns that actually exist in X\n",
    "numerical_cols = [c for c in numerical_cols if c in X.columns]\n",
    "binary_cols = [c for c in binary_cols if c in X.columns]\n",
    "ordinal_feature_names = [c for c in ordinal_feature_names if c in X.columns]\n",
    "nominal_cols = [c for c in nominal_cols if c in X.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "308c412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", RobustScaler())\n",
    "]) if numerical_cols else (\"num\", \"passthrough\", [])\n",
    "\n",
    "binary_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"binmap\", OrdinalEncoder(categories=[[\"No\",\"Yes\"]] * len(binary_cols)))\n",
    "]) if binary_cols else (\"bin\", \"passthrough\", [])\n",
    "\n",
    "ordinal_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ordenc\", OrdinalEncoder(categories=ordinal_categories))\n",
    "]) if ordinal_feature_names else (\"ord\", \"passthrough\", [])\n",
    "\n",
    "nominal_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(drop=\"first\", sparse_output=False))\n",
    "]) if nominal_cols else (\"nom\", \"passthrough\", [])\n",
    "\n",
    "# Create ColumnTransformer only with available transformers\n",
    "transformers = []\n",
    "if numerical_cols:\n",
    "    transformers.append((\"num\", numerical_pipeline, numerical_cols))\n",
    "if binary_cols:\n",
    "    transformers.append((\"bin\", binary_pipeline, binary_cols))\n",
    "if ordinal_feature_names:\n",
    "    transformers.append((\"ord\", ordinal_pipeline, ordinal_feature_names))\n",
    "if nominal_cols:\n",
    "    transformers.append((\"nom\", nominal_pipeline, nominal_cols))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d52d3c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shapes (full): (59598, 32) (14900, 32)\n",
      "Train shape (after split): (47678, 32)\n",
      "Val shape (after split): (11920, 32)\n"
     ]
    }
   ],
   "source": [
    "X_processed = preprocessor.fit_transform(X)            # numpy array, len = len(X)\n",
    "X_test = preprocessor.transform(X_test_final)          # test processed\n",
    "\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "X_cluster = pd.DataFrame(X_processed, index=X.index, columns=feature_names)\n",
    "X_test_cluster = pd.DataFrame(X_test, index=X_test_final.index, columns=feature_names)\n",
    "\n",
    "print(\"Processed shapes (full):\", X_cluster.shape, X_test_cluster.shape)\n",
    "\n",
    "X_train_idx, X_val_idx, y_train, y_val = train_test_split(\n",
    "    X.index, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train = X_cluster.loc[X_train_idx].reset_index(drop=True)\n",
    "X_val = X_cluster.loc[X_val_idx].reset_index(drop=True)\n",
    "y_train = y.loc[X_train_idx].reset_index(drop=True)\n",
    "y_val = y.loc[X_val_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"Train shape (after split):\", X_train.shape)\n",
    "print(\"Val shape (after split):\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "191d0ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding clustering features...\n",
      "New shapes (clustered): (59598, 39) (14900, 39)\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding clustering features...\")\n",
    "\n",
    "# Ensure column names are strings (fix for GMM)\n",
    "X_cluster.columns = X_cluster.columns.astype(str)\n",
    "X_test_cluster.columns = X_test_cluster.columns.astype(str)\n",
    "\n",
    "K = 12\n",
    "kmeans = MiniBatchKMeans(n_clusters=K, batch_size=4096, random_state=42)\n",
    "kmeans.fit(X_cluster)\n",
    "X_cluster[\"kmeans_label\"] = kmeans.labels_\n",
    "X_test_cluster[\"kmeans_label\"] = kmeans.predict(X_test_cluster)\n",
    "\n",
    "G = 6\n",
    "gmm = GaussianMixture(n_components=G, covariance_type=\"diag\", random_state=42)\n",
    "gmm.fit(X_cluster)\n",
    "gmm_train_proba = gmm.predict_proba(X_cluster)\n",
    "gmm_test_proba = gmm.predict_proba(X_test_cluster)\n",
    "\n",
    "for i in range(G):\n",
    "    X_cluster[f\"gmm_prob_{i}\"] = gmm_train_proba[:, i]\n",
    "    X_test_cluster[f\"gmm_prob_{i}\"] = gmm_test_proba[:, i]\n",
    "\n",
    "print(\"New shapes (clustered):\", X_cluster.shape, X_test_cluster.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cce9ac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN sample shapes: (11920, 39) (11920,)\n",
      "NN shapes (scaled): (9536, 39) (2384, 39)\n",
      "Epoch 1/50\n",
      "10/10 - 2s - 222ms/step - auc: 0.5935 - loss: 0.6845 - val_auc: 0.7504 - val_loss: 0.6234 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "10/10 - 0s - 21ms/step - auc: 0.7501 - loss: 0.6004 - val_auc: 0.7763 - val_loss: 0.5672 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "10/10 - 0s - 19ms/step - auc: 0.7865 - loss: 0.5573 - val_auc: 0.7895 - val_loss: 0.5577 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "10/10 - 0s - 20ms/step - auc: 0.7996 - loss: 0.5448 - val_auc: 0.7925 - val_loss: 0.5513 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "10/10 - 0s - 22ms/step - auc: 0.8024 - loss: 0.5397 - val_auc: 0.7924 - val_loss: 0.5503 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "10/10 - 0s - 23ms/step - auc: 0.8063 - loss: 0.5346 - val_auc: 0.7924 - val_loss: 0.5502 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "10/10 - 0s - 26ms/step - auc: 0.8108 - loss: 0.5300 - val_auc: 0.7925 - val_loss: 0.5520 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "10/10 - 0s - 29ms/step - auc: 0.8130 - loss: 0.5281 - val_auc: 0.7919 - val_loss: 0.5513 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "10/10 - 0s - 31ms/step - auc: 0.8149 - loss: 0.5253 - val_auc: 0.7925 - val_loss: 0.5508 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "10/10 - 0s - 33ms/step - auc: 0.8148 - loss: 0.5251 - val_auc: 0.7928 - val_loss: 0.5503 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "10/10 - 0s - 30ms/step - auc: 0.8135 - loss: 0.5256 - val_auc: 0.7933 - val_loss: 0.5491 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "10/10 - 0s - 31ms/step - auc: 0.8182 - loss: 0.5207 - val_auc: 0.7932 - val_loss: 0.5493 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "10/10 - 0s - 34ms/step - auc: 0.8197 - loss: 0.5179 - val_auc: 0.7929 - val_loss: 0.5499 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "10/10 - 0s - 34ms/step - auc: 0.8221 - loss: 0.5160 - val_auc: 0.7922 - val_loss: 0.5509 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "10/10 - 0s - 31ms/step - auc: 0.8213 - loss: 0.5162 - val_auc: 0.7923 - val_loss: 0.5505 - learning_rate: 2.5000e-04\n",
      "Epoch 16/50\n",
      "10/10 - 0s - 32ms/step - auc: 0.8238 - loss: 0.5139 - val_auc: 0.7923 - val_loss: 0.5506 - learning_rate: 2.5000e-04\n",
      "Epoch 17/50\n",
      "10/10 - 0s - 30ms/step - auc: 0.8204 - loss: 0.5172 - val_auc: 0.7924 - val_loss: 0.5508 - learning_rate: 2.5000e-04\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "NN Validation AUC: 0.7932894637511526\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "RND = 42\n",
    "frac = 0.20\n",
    "\n",
    "# keep IDCOL in train_fe for sampling (we used train.copy earlier so index aligns)\n",
    "if IDCOL not in train_fe.columns:\n",
    "    train_fe[IDCOL] = train[IDCOL].values\n",
    "\n",
    "nn_sample = train.sample(frac=frac, random_state=RND)  # sample rows (index preserved)\n",
    "nn_ids = nn_sample[IDCOL].tolist()\n",
    "\n",
    "# Create boolean mask aligned with X_cluster index (same as train_fe index)\n",
    "mask = train_fe[IDCOL].isin(nn_ids)\n",
    "\n",
    "# Apply mask to X_cluster and y (both indexed by train_fe index)\n",
    "X_nn = X_cluster[mask].reset_index(drop=True)\n",
    "y_nn = y[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"NN sample shapes:\", X_nn.shape, y_nn.shape)   # must match\n",
    "\n",
    "# If too small, fallback to using a random subset of X_cluster with same size\n",
    "if len(X_nn) < 10:\n",
    "    raise RuntimeError(\"NN sample too small — check sampling fraction and data.\")\n",
    "\n",
    "# Train/validation split for NN (within sampled set)\n",
    "X_nn_train, X_nn_val, y_nn_train, y_nn_val = train_test_split(\n",
    "    X_nn, y_nn, test_size=0.2, random_state=RND, stratify=y_nn\n",
    ")\n",
    "\n",
    "scaler_nn = StandardScaler()\n",
    "X_nn_train_s = scaler_nn.fit_transform(X_nn_train)\n",
    "X_nn_val_s   = scaler_nn.transform(X_nn_val)\n",
    "X_test_s_nn  = scaler_nn.transform(X_test_cluster)\n",
    "\n",
    "print(\"NN shapes (scaled):\", X_nn_train_s.shape, X_nn_val_s.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# Neural network model\n",
    "# ------------------------------\n",
    "def build_nn(input_dim, lr=1e-3, l2=1e-5, dropout=0.3):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.BatchNormalization()(inputs)\n",
    "    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x = layers.Dropout(dropout/2)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout/3)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[keras.metrics.AUC(name=\"auc\")])\n",
    "    return model\n",
    "\n",
    "nn_model = build_nn(X_nn_train_s.shape[1])\n",
    "\n",
    "early = callbacks.EarlyStopping(monitor=\"val_auc\", patience=6, mode=\"max\", restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, patience=3, mode=\"max\")\n",
    "\n",
    "history = nn_model.fit(\n",
    "    X_nn_train_s, y_nn_train,\n",
    "    validation_data=(X_nn_val_s, y_nn_val),\n",
    "    epochs=50,\n",
    "    batch_size=1024,\n",
    "    callbacks=[early, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "nn_val_probs = nn_model.predict(X_nn_val_s).ravel()\n",
    "nn_val_auc = roc_auc_score(y_nn_val, nn_val_probs)\n",
    "print(\"NN Validation AUC:\", nn_val_auc)\n",
    "\n",
    "nn_test_probs = nn_model.predict(X_test_s_nn).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea827206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Logistic Regression (FAST global search)...\n",
      "Best LR: {'C': 0.1, 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression tuning\n",
    "print(\"\\nTuning Logistic Regression (FAST global search)...\")\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=2000, random_state=42)\n",
    "params = {\n",
    "    'C': [0.01, 0.1, 1, 10, 50, 100],\n",
    "    'solver': ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Use X_cluster (full processed + clustering features) and y (aligned)\n",
    "grid = GridSearchCV(log_reg, params, cv=5, n_jobs=-1, scoring='roc_auc')\n",
    "grid.fit(X_cluster, y)\n",
    "best_lr = grid.best_estimator_\n",
    "print(\"Best LR:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d237437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FOLD 1 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 25007, number of negative: 22671\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 47678, number of used features: 39\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524498 -> initscore=0.098069\n",
      "[LightGBM] [Info] Start training from score 0.098069\n",
      "LGB AUC: 0.8203084224178006\n",
      "XGB AUC: 0.8177149681405087\n",
      "CAT AUC: 0.8235702130314664\n",
      "LR AUC: 0.8087863581612819\n",
      "\n",
      "--- FOLD 2 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 25028, number of negative: 22650\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 47678, number of used features: 39\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524938 -> initscore=0.099835\n",
      "[LightGBM] [Info] Start training from score 0.099835\n",
      "LGB AUC: 0.8472382740774443\n",
      "XGB AUC: 0.8460085580931593\n",
      "CAT AUC: 0.8513008039807247\n",
      "LR AUC: 0.8333682298973223\n",
      "\n",
      "--- FOLD 3 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 24957, number of negative: 22721\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2749\n",
      "[LightGBM] [Info] Number of data points in the train set: 47678, number of used features: 39\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523449 -> initscore=0.093865\n",
      "[LightGBM] [Info] Start training from score 0.093865\n",
      "LGB AUC: 0.8477922421709374\n",
      "XGB AUC: 0.8460041507796686\n",
      "CAT AUC: 0.8514706734285109\n",
      "LR AUC: 0.8329682469620411\n",
      "\n",
      "--- FOLD 4 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 24969, number of negative: 22710\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 47679, number of used features: 39\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523690 -> initscore=0.094830\n",
      "[LightGBM] [Info] Start training from score 0.094830\n",
      "LGB AUC: 0.8511033010798132\n",
      "XGB AUC: 0.8480283342693395\n",
      "CAT AUC: 0.8543697198545276\n",
      "LR AUC: 0.8354017827839706\n",
      "\n",
      "--- FOLD 5 ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 25079, number of negative: 22600\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2749\n",
      "[LightGBM] [Info] Number of data points in the train set: 47679, number of used features: 39\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525997 -> initscore=0.104081\n",
      "[LightGBM] [Info] Start training from score 0.104081\n",
      "LGB AUC: 0.8453222918771583\n",
      "XGB AUC: 0.8434651490764065\n",
      "CAT AUC: 0.8489803555335957\n",
      "LR AUC: 0.8295778070272244\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NFOLDS = 5\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=RND)\n",
    "\n",
    "# storage\n",
    "oof_lgb = np.zeros(len(X_cluster))\n",
    "oof_xgb = np.zeros(len(X_cluster))\n",
    "oof_cat = np.zeros(len(X_cluster))\n",
    "oof_lr  = np.zeros(len(X_cluster))\n",
    "\n",
    "test_lgb = np.zeros(len(X_test_cluster))\n",
    "test_xgb = np.zeros(len(X_test_cluster))\n",
    "test_cat = np.zeros(len(X_test_cluster))\n",
    "test_lr  = np.zeros(len(X_test_cluster))\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# MODEL PARAMETERS\n",
    "# -------------------------\n",
    "lgb_params = dict(\n",
    "    n_estimators=500, learning_rate=0.05,\n",
    "    num_leaves=64, subsample=0.8,\n",
    "    colsample_bytree=0.8, random_state=42\n",
    ")\n",
    "\n",
    "xgb_params = dict(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"auc\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cat_params = dict(\n",
    "    iterations=500, learning_rate=0.05,\n",
    "    depth=6, random_seed=42, verbose=0\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# K-FOLD TRAINING\n",
    "# -------------------------\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_cluster, y), 1):\n",
    "    print(f\"\\n--- FOLD {fold} ---\")\n",
    "    X_tr, X_val = X_cluster.iloc[tr_idx], X_cluster.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "\n",
    "    # -------------------------\n",
    "    # LIGHTGBM\n",
    "    # -------------------------\n",
    "    lgbm = lgb.LGBMClassifier(**lgb_params)\n",
    "    lgbm.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    oof_lgb[val_idx] = lgbm.predict_proba(X_val)[:,1]\n",
    "    test_lgb += lgbm.predict_proba(X_test_cluster)[:,1] / NFOLDS\n",
    "    print(\"LGB AUC:\", roc_auc_score(y_val, oof_lgb[val_idx]))\n",
    "\n",
    "    # -------------------------\n",
    "    # XGBOOST (train full n_estimators)\n",
    "    # -------------------------\n",
    "    xgbm = XGBClassifier(**xgb_params)\n",
    "    xgbm.fit(X_tr, y_tr)  # no early stopping\n",
    "    oof_xgb[val_idx] = xgbm.predict_proba(X_val)[:,1]\n",
    "    test_xgb += xgbm.predict_proba(X_test_cluster)[:,1] / NFOLDS\n",
    "    print(\"XGB AUC:\", roc_auc_score(y_val, oof_xgb[val_idx]))\n",
    "\n",
    "    # -------------------------\n",
    "    # CATBOOST\n",
    "    # -------------------------\n",
    "    cat = CatBoostClassifier(**cat_params)\n",
    "    cat.fit(X_tr, y_tr, eval_set=(X_val, y_val), verbose=False)\n",
    "    oof_cat[val_idx] = cat.predict_proba(X_val)[:,1]\n",
    "    test_cat += cat.predict_proba(X_test_cluster)[:,1] / NFOLDS\n",
    "    print(\"CAT AUC:\", roc_auc_score(y_val, oof_cat[val_idx]))\n",
    "\n",
    "    # -------------------------\n",
    "    # LOGISTIC REGRESSION\n",
    "    # -------------------------\n",
    "    lr_model.fit(X_tr, y_tr)\n",
    "    oof_lr[val_idx] = lr_model.predict_proba(X_val)[:,1]\n",
    "    test_lr += lr_model.predict_proba(X_test_cluster)[:,1] / NFOLDS\n",
    "    print(\"LR AUC:\", roc_auc_score(y_val, oof_lr[val_idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2b23e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OOF LGB: 0.8404795775536034\n",
      "OOF XGB: 0.8381304392362247\n",
      "OOF CAT: 0.844350728368235\n",
      "OOF LR : 0.8275043803330665\n",
      "NN VAL : 0.7932894637511526\n"
     ]
    }
   ],
   "source": [
    "# Compute OOF AUC\n",
    "auc_lgb = roc_auc_score(y, oof_lgb)\n",
    "auc_xgb = roc_auc_score(y, oof_xgb)\n",
    "auc_cat = roc_auc_score(y, oof_cat)\n",
    "auc_lr  = roc_auc_score(y, oof_lr)\n",
    "\n",
    "print(\"\\nOOF LGB:\", auc_lgb)\n",
    "print(\"OOF XGB:\", auc_xgb)\n",
    "print(\"OOF CAT:\", auc_cat)\n",
    "print(\"OOF LR :\", auc_lr)\n",
    "print(\"NN VAL :\", nn_val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0719c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Blend Weights: [0.20283044 0.20226353 0.20376466 0.19969918 0.19144219]\n",
      "\n",
      "Submission saved as submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Blend weights\n",
    "raw = np.array([auc_lgb, auc_xgb, auc_cat, auc_lr, nn_val_auc])\n",
    "# guard in case any AUC is NaN\n",
    "raw = np.nan_to_num(raw, nan=0.0)\n",
    "if raw.sum() <= 0:\n",
    "    # fallback to equal weights\n",
    "    weights = np.ones(len(raw)) / len(raw)\n",
    "else:\n",
    "    weights = raw / raw.sum()\n",
    "print(\"\\nBlend Weights:\", weights)\n",
    "\n",
    "# Blended test predictions\n",
    "test_blend = (\n",
    "    weights[0]*test_lgb +\n",
    "    weights[1]*test_xgb +\n",
    "    weights[2]*test_cat +\n",
    "    weights[3]*test_lr +\n",
    "    weights[4]*nn_test_probs\n",
    ")\n",
    "\n",
    "# Save submission\n",
    "submission = pd.DataFrame({\n",
    "    IDCOL: test[IDCOL] if IDCOL in test.columns else np.arange(len(test)),\n",
    "    TARGET: np.where(test_blend >= 0.5, \"Stayed\", \"Left\")\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSubmission saved as submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
