{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea888a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57543a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../train.csv\")\n",
    "test = pd.read_csv(\"../test.csv\")\n",
    "\n",
    "TARGET = \"retention_status\"\n",
    "IDCOL = \"founder_id\"\n",
    "\n",
    "# Remove duplicates\n",
    "train = train.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b978cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cols = ['monthly_revenue_generated', 'funding_rounds_led', 'num_dependents', 'years_with_startup']\n",
    "\n",
    "def feature_engineer(df):\n",
    "    df = df.copy()\n",
    "    if 'years_with_startup' in df and 'years_since_founding' in df:\n",
    "        df['experience_ratio'] = df['years_with_startup'] / (df['years_since_founding'] + 1e-9)\n",
    "    if 'founder_age' in df and 'years_with_startup' in df:\n",
    "        df['founder_join_age'] = df['founder_age'] - df['years_with_startup']\n",
    "    if 'monthly_revenue_generated' in df and 'funding_rounds_led' in df:\n",
    "        df['revenue_per_round'] = df['monthly_revenue_generated'] / (df['funding_rounds_led'] + 1)\n",
    "    for c in log_cols:\n",
    "        if c in df:\n",
    "            df[f\"log_{c}\"] = np.log1p(df[c])\n",
    "            df.drop(c, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_fe = feature_engineer(train)\n",
    "test_fe = feature_engineer(test)\n",
    "\n",
    "if IDCOL not in test_fe.columns and IDCOL in test.columns:\n",
    "    test_fe[IDCOL] = test[IDCOL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87af4b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_fe.drop(columns=[TARGET, IDCOL])\n",
    "y = train_fe[TARGET].map({\"Stayed\": 1, \"Left\": 0}).astype(int)\n",
    "X_test = test_fe.drop(columns=[IDCOL], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70fb5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed X shape: (59598, 32) X_test shape: (14900, 32)\n"
     ]
    }
   ],
   "source": [
    "# Feature columns\n",
    "numerical_cols = [c for c in [\n",
    "    'years_since_founding', 'founder_age', 'distance_from_investor_hub',\n",
    "    'experience_ratio', 'founder_join_age', 'revenue_per_round',\n",
    "    'log_monthly_revenue_generated', 'log_funding_rounds_led',\n",
    "    'log_num_dependents', 'log_years_with_startup'\n",
    "] if c in X.columns]\n",
    "\n",
    "binary_cols = [c for c in [\n",
    "    'working_overtime', 'remote_operations',\n",
    "    'leadership_scope', 'innovation_support'\n",
    "] if c in X.columns]\n",
    "\n",
    "ordinal_cols = {\n",
    "    'work_life_balance_rating': ['Poor', 'Fair', 'Good', 'Excellent'],\n",
    "    'venture_satisfaction': ['Low', 'Medium', 'High', 'Very High'],\n",
    "    'startup_performance_rating': ['Low', 'Below Average', 'Average', 'High'],\n",
    "    'startup_reputation': ['Poor', 'Fair', 'Good', 'Excellent'],\n",
    "    'founder_visibility': ['Low', 'Medium', 'High', 'Very High'],\n",
    "    'startup_stage': ['Entry', 'Mid', 'Senior'],\n",
    "    'team_size_category': ['Small', 'Medium', 'Large']\n",
    "}\n",
    "ordinal_feature_names = [c for c in ordinal_cols.keys() if c in X.columns]\n",
    "ordinal_categories = [ordinal_cols[c] for c in ordinal_feature_names]\n",
    "\n",
    "nominal_cols = [c for c in ['founder_gender', 'founder_role', 'education_background', 'personal_status'] if c in X.columns]\n",
    "\n",
    "# Pipelines\n",
    "transformers = []\n",
    "\n",
    "if numerical_cols:\n",
    "    num_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', RobustScaler())\n",
    "    ])\n",
    "    transformers.append(('num', num_pipe, numerical_cols))\n",
    "\n",
    "if binary_cols:\n",
    "    bin_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OrdinalEncoder(categories=[['No','Yes']]*len(binary_cols)))\n",
    "    ])\n",
    "    transformers.append(('bin', bin_pipe, binary_cols))\n",
    "\n",
    "if ordinal_feature_names:\n",
    "    ord_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OrdinalEncoder(categories=ordinal_categories))\n",
    "    ])\n",
    "    transformers.append(('ord', ord_pipe, ordinal_feature_names))\n",
    "\n",
    "if nominal_cols:\n",
    "    nom_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse_output=False))\n",
    "    ])\n",
    "    transformers.append(('nom', nom_pipe, nominal_cols))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "X = pd.DataFrame(preprocessor.fit_transform(X), columns=preprocessor.get_feature_names_out())\n",
    "X_test = pd.DataFrame(preprocessor.transform(X_test), columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "print(\"Processed X shape:\", X.shape, \"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae299931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels added!\n"
     ]
    }
   ],
   "source": [
    "K = 12\n",
    "kmeans = MiniBatchKMeans(n_clusters=K, batch_size=4096, random_state=42)\n",
    "kmeans.fit(X)\n",
    "X[\"cluster_label\"] = kmeans.labels_\n",
    "X_test[\"cluster_label\"] = kmeans.predict(X_test)\n",
    "\n",
    "print(\"Cluster labels added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a9fdc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold setup\n",
    "NFOLDS = 5\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_lr = np.zeros(len(X))\n",
    "oof_lgb = np.zeros(len(X))\n",
    "oof_xgb = np.zeros(len(X))\n",
    "oof_cat = np.zeros(len(X))\n",
    "oof_mlp = np.zeros(len(X))\n",
    "\n",
    "test_lr = np.zeros(len(X_test))\n",
    "test_lgb = np.zeros(len(X_test))\n",
    "test_xgb = np.zeros(len(X_test))\n",
    "test_cat = np.zeros(len(X_test))\n",
    "test_mlp = np.zeros(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d14bcda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_template = LogisticRegression(max_iter=2000, solver='lbfgs', class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "lgb_params = dict(n_estimators=800, learning_rate=0.05, num_leaves=64, max_depth=-1, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, verbose=-1)\n",
    "xgb_params = dict(n_estimators=800, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "cat_params = dict(iterations=800, learning_rate=0.05, depth=6, random_seed=42, verbose=0)\n",
    "\n",
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "352fbfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_mlp(X_tr, y_tr, X_val):\n",
    "    X_tr = torch.tensor(X_tr.values, dtype=torch.float32)\n",
    "    y_tr = torch.tensor(y_tr.values.reshape(-1,1), dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    model = MLP(X_tr.shape[1])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    model.train()\n",
    "    for epoch in range(15):\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_tr)\n",
    "        loss = loss_fn(preds, y_tr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_val).numpy().flatten()\n",
    "    return model, val_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e5bd17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FOLD 1 ===\n",
      "LR AUC: 0.8088208431084188\n",
      "LGB AUC: 0.8152529630316006\n",
      "XGB AUC: 0.8144807062223685\n",
      "CAT AUC: 0.8218376971889209\n",
      "MLP AUC: 0.7348986761985093\n",
      "\n",
      "=== FOLD 2 ===\n",
      "LR AUC: 0.8333668757865127\n",
      "LGB AUC: 0.8409727751508028\n",
      "XGB AUC: 0.842338635692736\n",
      "CAT AUC: 0.8489692508517357\n",
      "MLP AUC: 0.7679057175523454\n",
      "\n",
      "=== FOLD 3 ===\n",
      "LR AUC: 0.8329919166366488\n",
      "LGB AUC: 0.8442913052274872\n",
      "XGB AUC: 0.8433421315039106\n",
      "CAT AUC: 0.8495660272493316\n",
      "MLP AUC: 0.7826053369015227\n",
      "\n",
      "=== FOLD 4 ===\n",
      "LR AUC: 0.8354715737117034\n",
      "LGB AUC: 0.8450837135258378\n",
      "XGB AUC: 0.8448763743107475\n",
      "CAT AUC: 0.8514871653043457\n",
      "MLP AUC: 0.7209838498539842\n",
      "\n",
      "=== FOLD 5 ===\n",
      "LR AUC: 0.8296824971385737\n",
      "LGB AUC: 0.8394296173710358\n",
      "XGB AUC: 0.8401831295931623\n",
      "CAT AUC: 0.8477735292082591\n",
      "MLP AUC: 0.7517334347847149\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y), 1):\n",
    "    print(f\"\\n=== FOLD {fold} ===\")\n",
    "    X_tr, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    # Logistic Regression\n",
    "    X_tr_s = scaler.fit_transform(X_tr)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "    lr = LogisticRegression(**lr_model_template.get_params())\n",
    "    lr.fit(X_tr_s, y_tr)\n",
    "    oof_lr[valid_idx] = lr.predict_proba(X_val_s)[:,1]\n",
    "    test_lr += lr.predict_proba(scaler.transform(X_test))[:,1] / NFOLDS\n",
    "    print(\"LR AUC:\", roc_auc_score(y_val, oof_lr[valid_idx]))\n",
    "\n",
    "    # LightGBM\n",
    "    lgbm = lgb.LGBMClassifier(**lgb_params)\n",
    "    lgbm.fit(X_tr, y_tr)\n",
    "    oof_lgb[valid_idx] = lgbm.predict_proba(X_val)[:,1]\n",
    "    test_lgb += lgbm.predict_proba(X_test)[:,1] / NFOLDS\n",
    "    print(\"LGB AUC:\", roc_auc_score(y_val, oof_lgb[valid_idx]))\n",
    "\n",
    "    # XGBoost\n",
    "    xgb = XGBClassifier(**xgb_params)\n",
    "    xgb.fit(X_tr, y_tr)\n",
    "    oof_xgb[valid_idx] = xgb.predict_proba(X_val)[:,1]\n",
    "    test_xgb += xgb.predict_proba(X_test)[:,1] / NFOLDS\n",
    "    print(\"XGB AUC:\", roc_auc_score(y_val, oof_xgb[valid_idx]))\n",
    "\n",
    "    # CatBoost\n",
    "    cat = CatBoostClassifier(**cat_params)\n",
    "    cat.fit(X_tr, y_tr)\n",
    "    oof_cat[valid_idx] = cat.predict_proba(X_val)[:,1]\n",
    "    test_cat += cat.predict_proba(X_test)[:,1] / NFOLDS\n",
    "    print(\"CAT AUC:\", roc_auc_score(y_val, oof_cat[valid_idx]))\n",
    "\n",
    "    # MLP\n",
    "    mlp_model, val_mlp = train_mlp(X_tr, y_tr, X_val)\n",
    "    oof_mlp[valid_idx] = val_mlp\n",
    "    with torch.no_grad():\n",
    "        test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "        test_mlp += mlp_model(test_tensor).numpy().flatten() / NFOLDS\n",
    "    print(\"MLP AUC:\", roc_auc_score(y_val, oof_mlp[valid_idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9471756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "META OOF AUC: 0.8424858407649873\n",
      "Submission saved!\n"
     ]
    }
   ],
   "source": [
    "# Stack predictions\n",
    "stack_train = pd.DataFrame({\n",
    "    \"lr\": oof_lr,\n",
    "    \"lgb\": oof_lgb,\n",
    "    \"xgb\": oof_xgb,\n",
    "    \"cat\": oof_cat,\n",
    "    \"mlp\": oof_mlp\n",
    "})\n",
    "stack_test = pd.DataFrame({\n",
    "    \"lr\": test_lr,\n",
    "    \"lgb\": test_lgb,\n",
    "    \"xgb\": test_xgb,\n",
    "    \"cat\": test_cat,\n",
    "    \"mlp\": test_mlp\n",
    "})\n",
    "\n",
    "# Meta-model\n",
    "meta = LogisticRegression(max_iter=2000, solver='lbfgs', random_state=42)\n",
    "meta.fit(stack_train, y)\n",
    "meta_oof = meta.predict_proba(stack_train)[:,1]\n",
    "print(\"META OOF AUC:\", roc_auc_score(y, meta_oof))\n",
    "\n",
    "# Final predictions\n",
    "final_probs = meta.predict_proba(stack_test)[:,1]\n",
    "final_labels = np.where(final_probs >= 0.5, \"Stayed\", \"Left\")\n",
    "submission = pd.DataFrame({IDCOL: test[IDCOL] if IDCOL in test.columns else np.arange(len(test)), TARGET: final_labels})\n",
    "submission.to_csv(\"submission_safe_stack.csv\", index=False)\n",
    "print(\"Submission saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
