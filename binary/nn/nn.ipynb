{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55d60f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, RobustScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "try:\n",
    "    \n",
    "    from kerastuner.tuners import BayesianOptimization\n",
    "    from kerastuner.engine.hyperparameters import HyperParameters\n",
    "    KT_BACKEND = 'kerastuner'\n",
    "except Exception:\n",
    "    import keras_tuner as kt\n",
    "    from keras_tuner.tuners import BayesianOptimization\n",
    "    from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "    KT_BACKEND = 'keras_tuner'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # silence TF logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6796002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows in train: 13\n",
      "Duplicates removed. New shape: (59598, 24)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../train.csv\")\n",
    "test = pd.read_csv(\"../test.csv\")\n",
    "\n",
    "\n",
    "dup_count = train.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows in train: {dup_count}\")\n",
    "\n",
    "if dup_count > 0:\n",
    "    train.drop_duplicates(inplace=True)\n",
    "    print(\"Duplicates removed. New shape:\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8baa9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transform_cols = [\n",
    "    'monthly_revenue_generated', 'funding_rounds_led',\n",
    "    'num_dependents', 'years_with_startup'\n",
    "]\n",
    "\n",
    "def feature_engineer(df):\n",
    "    df = df.copy()\n",
    "    if 'years_with_startup' in df.columns and 'years_since_founding' in df.columns:\n",
    "        df['experience_ratio'] = df['years_with_startup'] / (df['years_since_founding'] + np.finfo(float).eps)\n",
    "    if 'founder_age' in df.columns and 'years_with_startup' in df.columns:\n",
    "        df['founder_join_age'] = df['founder_age'] - df['years_with_startup']\n",
    "    if 'monthly_revenue_generated' in df.columns and 'funding_rounds_led' in df.columns:\n",
    "        df['revenue_per_round'] = df['monthly_revenue_generated'] / (df['funding_rounds_led'] + 1)\n",
    "    for col in log_transform_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'log_{col}'] = np.log1p(df[col])\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_fe = feature_engineer(train.drop(columns=['founder_id']))\n",
    "test_fe = feature_engineer(test.drop(columns=['founder_id']))\n",
    "\n",
    "X = train_fe.drop(columns=['retention_status'])\n",
    "y = train_fe['retention_status'].map({'Stayed': 1, 'Left': 0}).astype(int)\n",
    "X_test_final = test_fe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e938ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [\n",
    "    'years_since_founding', 'founder_age', 'distance_from_investor_hub',\n",
    "    'experience_ratio', 'founder_join_age', 'revenue_per_round',\n",
    "    'log_monthly_revenue_generated', 'log_funding_rounds_led',\n",
    "    'log_num_dependents', 'log_years_with_startup'\n",
    "]\n",
    "\n",
    "binary_cols = ['working_overtime', 'remote_operations', 'leadership_scope', 'innovation_support']\n",
    "\n",
    "ordinal_cols = {\n",
    "    'work_life_balance_rating': ['Poor', 'Fair', 'Good', 'Excellent'],\n",
    "    'venture_satisfaction': ['Low', 'Medium', 'High', 'Very High'],\n",
    "    'startup_performance_rating': ['Low', 'Below Average', 'Average', 'High'],\n",
    "    'startup_reputation': ['Poor', 'Fair', 'Good', 'Excellent'],\n",
    "    'founder_visibility': ['Low', 'Medium', 'High', 'Very High'],\n",
    "    'startup_stage': ['Entry', 'Mid', 'Senior'],\n",
    "    'team_size_category': ['Small', 'Medium', 'Large']\n",
    "}\n",
    "ordinal_feature_names = list(ordinal_cols.keys())\n",
    "ordinal_categories = list(ordinal_cols.values())\n",
    "\n",
    "nominal_cols = ['founder_gender', 'founder_role', 'education_background', 'personal_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9211e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "binary_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('mapper', OrdinalEncoder(categories=[['No', 'Yes']] * len(binary_cols),\n",
    "                              handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "ordinal_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal_encoder', OrdinalEncoder(categories=ordinal_categories, handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "nominal_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_cols),\n",
    "        ('bin', binary_pipeline, binary_cols),\n",
    "        ('ord', ordinal_pipeline, ordinal_feature_names),\n",
    "        ('nom', nominal_pipeline, nominal_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0672cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw train/val sizes: (47678, 25), (11920, 25)\n",
      "Processed shapes (train/val/test): (47678, 32) (11920, 32) (14900, 32)\n"
     ]
    }
   ],
   "source": [
    "X_train_df, X_val_df, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Raw train/val sizes: {X_train_df.shape}, {X_val_df.shape}\")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train_df)\n",
    "X_val_processed = preprocessor.transform(X_val_df)\n",
    "X_test_processed = preprocessor.transform(X_test_final)\n",
    "\n",
    "print(\"Processed shapes (train/val/test):\", X_train_processed.shape, X_val_processed.shape, X_test_processed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dfb287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model(hp, input_dim):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(input_dim,)))\n",
    "    # first dense\n",
    "    model.add(keras.layers.Dense(units=hp.Int('units_1', 64, 512, step=64), activation='relu'))\n",
    "    # additional layers (0-2)\n",
    "    for i in range(hp.Int('num_layers', 0, 2)):\n",
    "        model.add(keras.layers.Dense(units=hp.Int(f'units_{i+2}', 32, 256, step=32), activation='relu'))\n",
    "        if hp.Boolean(f'dropout_{i}', default=False):\n",
    "            model.add(keras.layers.Dropout(rate=hp.Float(f'dropout_rate_{i}', 0.1, 0.5, step=0.1)))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    # important: name the AUC metric 'auc' so tuner sees 'val_auc'\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc'), 'accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e96ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bayesian_tuner(input_dim, project_name, max_trials=8, executions_per_trial=1):\n",
    "    def model_builder(hp):\n",
    "        return build_keras_model(hp, input_dim=input_dim)\n",
    "    # For kerastuner compatibility, objective can be 'val_auc'\n",
    "    tuner = BayesianOptimization(\n",
    "        model_builder,\n",
    "        objective='val_auc',\n",
    "        max_trials=max_trials,\n",
    "        executions_per_trial=executions_per_trial,\n",
    "        directory='kt_dir',\n",
    "        project_name=project_name,\n",
    "        seed=42\n",
    "    )\n",
    "    return tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "199257fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Complete [00h 00m 22s]\n",
      "val_auc: 0.8328278064727783\n",
      "\n",
      "Best val_auc So Far: 0.8356896638870239\n",
      "Total elapsed time: 00h 14m 20s\n",
      "Best hyperparameters (no-sample): {'units_1': 320, 'num_layers': 0, 'learning_rate': 0.001, 'units_2': 64, 'dropout_0': True, 'units_3': 160, 'dropout_1': True}\n",
      "Epoch 1/15\n",
      "420/420 - 3s - 6ms/step - accuracy: 0.7295 - auc: 0.8115 - loss: 0.5252 - val_accuracy: 0.7369 - val_auc: 0.8264 - val_loss: 0.5148\n",
      "Epoch 2/15\n",
      "420/420 - 1s - 3ms/step - accuracy: 0.7404 - auc: 0.8280 - loss: 0.5049 - val_accuracy: 0.7424 - val_auc: 0.8286 - val_loss: 0.5097\n",
      "Epoch 3/15\n",
      "420/420 - 1s - 2ms/step - accuracy: 0.7440 - auc: 0.8314 - loss: 0.5005 - val_accuracy: 0.7409 - val_auc: 0.8306 - val_loss: 0.5024\n",
      "Epoch 4/15\n",
      "420/420 - 1s - 2ms/step - accuracy: 0.7453 - auc: 0.8333 - loss: 0.4978 - val_accuracy: 0.7406 - val_auc: 0.8322 - val_loss: 0.4990\n",
      "Epoch 5/15\n",
      "420/420 - 1s - 3ms/step - accuracy: 0.7477 - auc: 0.8359 - loss: 0.4944 - val_accuracy: 0.7448 - val_auc: 0.8339 - val_loss: 0.4963\n",
      "Epoch 6/15\n",
      "420/420 - 1s - 3ms/step - accuracy: 0.7486 - auc: 0.8374 - loss: 0.4923 - val_accuracy: 0.7445 - val_auc: 0.8328 - val_loss: 0.4994\n",
      "Epoch 7/15\n",
      "420/420 - 1s - 3ms/step - accuracy: 0.7500 - auc: 0.8393 - loss: 0.4898 - val_accuracy: 0.7456 - val_auc: 0.8351 - val_loss: 0.4963\n",
      "Epoch 8/15\n",
      "420/420 - 1s - 2ms/step - accuracy: 0.7520 - auc: 0.8410 - loss: 0.4873 - val_accuracy: 0.7362 - val_auc: 0.8351 - val_loss: 0.5135\n",
      "Epoch 9/15\n",
      "420/420 - 1s - 2ms/step - accuracy: 0.7516 - auc: 0.8416 - loss: 0.4863 - val_accuracy: 0.7446 - val_auc: 0.8351 - val_loss: 0.4959\n",
      "Epoch 10/15\n",
      "420/420 - 1s - 3ms/step - accuracy: 0.7534 - auc: 0.8435 - loss: 0.4837 - val_accuracy: 0.7456 - val_auc: 0.8342 - val_loss: 0.4962\n",
      "Epoch 11/15\n",
      "420/420 - 1s - 2ms/step - accuracy: 0.7529 - auc: 0.8449 - loss: 0.4818 - val_accuracy: 0.7495 - val_auc: 0.8362 - val_loss: 0.4937\n",
      "Epoch 12/15\n",
      "420/420 - 1s - 2ms/step - accuracy: 0.7548 - auc: 0.8459 - loss: 0.4802 - val_accuracy: 0.7435 - val_auc: 0.8356 - val_loss: 0.4943\n",
      "Epoch 13/15\n",
      "420/420 - 1s - 2ms/step - accuracy: 0.7569 - auc: 0.8473 - loss: 0.4784 - val_accuracy: 0.7450 - val_auc: 0.8358 - val_loss: 0.4962\n",
      "Epoch 14/15\n",
      "420/420 - 1s - 2ms/step - accuracy: 0.7580 - auc: 0.8482 - loss: 0.4771 - val_accuracy: 0.7460 - val_auc: 0.8363 - val_loss: 0.4947\n",
      "Epoch 15/15\n",
      "420/420 - 1s - 2ms/step - accuracy: 0.7582 - auc: 0.8490 - loss: 0.4758 - val_accuracy: 0.7455 - val_auc: 0.8351 - val_loss: 0.4958\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Saved submission_nn_nosample.csv\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "PATIENCE = 3\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_auc', patience=PATIENCE, mode='max', restore_best_weights=True, verbose=1)\n",
    "\n",
    "def probs_to_labels(probs, thresh=0.5):\n",
    "    return np.where(probs.ravel() >= thresh, 'Stayed', 'Left')\n",
    "\n",
    "# ------------------ MODEL A: No Sampling ------------------\n",
    "print(\"\\n===== MODEL A (No Sampling) =====\")\n",
    "input_dim = X_train_processed.shape[1]\n",
    "tuner_a = get_bayesian_tuner(input_dim=input_dim, project_name='nn_nosample', max_trials=8, executions_per_trial=1)\n",
    "\n",
    "# compute class weights for imbalance\n",
    "classes = np.unique(y_train)\n",
    "class_weights_vals = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "cw = {int(c): float(w) for c, w in zip(classes, class_weights_vals)}\n",
    "\n",
    "# search\n",
    "tuner_a.search(\n",
    "    x=X_train_processed,\n",
    "    y=y_train,\n",
    "    validation_data=(X_val_processed, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "best_hps_a = tuner_a.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters (no-sample):\", best_hps_a.values)\n",
    "\n",
    "# build best model and final fit on combined train+val\n",
    "best_model_a = tuner_a.hypermodel.build(best_hps_a)\n",
    "X_full = np.vstack([X_train_processed, X_val_processed])\n",
    "y_full = np.concatenate([y_train.values, y_val.values])\n",
    "\n",
    "best_model_a.fit(\n",
    "    X_full, y_full,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    validation_split=0.1,\n",
    "    class_weight=cw,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "probs_a = best_model_a.predict(X_test_processed, batch_size=BATCH_SIZE, verbose=0)\n",
    "labels_a = probs_to_labels(probs_a)\n",
    "pd.DataFrame({'founder_id': test['founder_id'], 'retention_status': labels_a}).to_csv('submission_nn_nosample.csv', index=False)\n",
    "print(\"Saved submission_nn_nosample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16dbdd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Complete [00h 00m 09s]\n",
      "val_auc: 0.8304513692855835\n",
      "\n",
      "Best val_auc So Far: 0.831977367401123\n",
      "Total elapsed time: 00h 00m 55s\n",
      "Best hyperparameters (SMOTE): {'units_1': 512, 'num_layers': 1, 'learning_rate': 0.001, 'units_2': 128, 'dropout_0': False, 'units_3': 32, 'dropout_1': True}\n",
      "Epoch 1/15\n",
      "436/436 - 3s - 7ms/step - accuracy: 0.7310 - auc: 0.8163 - loss: 0.5195 - val_accuracy: 0.7389 - val_auc: 0.8287 - val_loss: 0.5035\n",
      "Epoch 2/15\n",
      "436/436 - 2s - 3ms/step - accuracy: 0.7438 - auc: 0.8308 - loss: 0.5010 - val_accuracy: 0.7370 - val_auc: 0.8308 - val_loss: 0.5040\n",
      "Epoch 3/15\n",
      "436/436 - 2s - 4ms/step - accuracy: 0.7481 - auc: 0.8360 - loss: 0.4930 - val_accuracy: 0.7438 - val_auc: 0.8317 - val_loss: 0.4993\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Saved submission_nn_smote.csv\n",
      "\n",
      "===== ALL DONE =====\n",
      "Files created: submission_nn_nosample.csv, submission_nn_smote.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== MODEL B (SMOTE) =====\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_sm, y_sm = smote.fit_resample(X_train_processed, y_train)\n",
    "print(\"After SMOTE:\", X_sm.shape, y_sm.shape)\n",
    "\n",
    "tuner_b = get_bayesian_tuner(input_dim=input_dim, project_name='nn_smote', max_trials=8, executions_per_trial=1)\n",
    "\n",
    "tuner_b.search(\n",
    "    x=X_sm,\n",
    "    y=y_sm,\n",
    "    validation_data=(X_val_processed, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "best_hps_b = tuner_b.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters (SMOTE):\", best_hps_b.values)\n",
    "\n",
    "best_model_b = tuner_b.hypermodel.build(best_hps_b)\n",
    "\n",
    "# final fit on SMOTE train + val combined\n",
    "X_combined = np.vstack([X_sm, X_val_processed])\n",
    "y_combined = np.concatenate([y_sm, y_val.values])\n",
    "\n",
    "best_model_b.fit(\n",
    "    X_combined, y_combined,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    validation_split=0.1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "probs_b = best_model_b.predict(X_test_processed, batch_size=BATCH_SIZE, verbose=0)\n",
    "labels_b = probs_to_labels(probs_b)\n",
    "pd.DataFrame({'founder_id': test['founder_id'], 'retention_status': labels_b}).to_csv('submission_nn_smote.csv', index=False)\n",
    "print(\"Saved submission_nn_smote.csv\")\n",
    "\n",
    "print(\"\\n===== ALL DONE =====\")\n",
    "print(\"Files created: submission_nn_nosample.csv, submission_nn_smote.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
